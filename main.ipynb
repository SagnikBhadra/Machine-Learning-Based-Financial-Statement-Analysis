{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7321695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import argparse\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea0fc5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class OLS():\n",
    "    def __init__(self) -> None:\n",
    "        super(OLS, self).__init__()\n",
    "        self.ols = LinearRegression()\n",
    "\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.ols.fit(x_train, y_train)\n",
    "        filename = 'trained_models/ols_model_mse.sav'\n",
    "        pickle.dump(self.ols, open(filename, 'wb'))\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.ols = model\n",
    "\n",
    "    def test(self, x_test):\n",
    "        return self.ols.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da29ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "class LASSO():\n",
    "    def __init__(self) -> None:\n",
    "        super(LASSO, self).__init__()\n",
    "        self.lasso = Lasso(alpha= 1)\n",
    "\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.lasso.fit(x_train, y_train)\n",
    "        filename = 'trained_models/lasso_model_mse.sav'\n",
    "        pickle.dump(self.lasso, open(filename, 'wb'))\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.lasso = model\n",
    "\n",
    "    def test(self, x_test):\n",
    "        return self.lasso.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb62867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self):\n",
    "        #CART tree\n",
    "        #num_tress_in_forest = 200\n",
    "        #Consider depth of tree hyperparameter\n",
    "\n",
    "        self.n_estimators = 50\n",
    "        self.random_forest = RandomForestRegressor(n_estimators = self.n_estimators)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.random_forest_fitted = self.random_forest.fit(x_train, y_train)\n",
    "        filename = 'trained_models/random_forest_model_mse.sav'\n",
    "        pickle.dump(self.random_forest_fitted, open(filename, 'wb'))\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.random_forest_fitted = model\n",
    "\n",
    "    def test(self, x_test):\n",
    "        y_predictions = self.random_forest_fitted.predict(x_test)\n",
    "        return y_predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b9181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        #The deep neural network is initalized as mentioned in the paper\n",
    "        #Use the Adam optimizer\n",
    "        #Learning rate = 0.00005\n",
    "        #Use batchnorm\n",
    "        #Input layer dim = 484 (4 concatenated quarters), hidden layers dim = (100, 50, 33) \n",
    "        #Activation funciton = Exponential Linear Unit (ELU)\n",
    "        #Batch size = 256\n",
    "        #Epochs = 10\n",
    "\n",
    "        self.LinearLayer1 = nn.Linear(484, 100)\n",
    "        self.BN1 = nn.BatchNorm1d(100)\n",
    "        self.LinearLayer2 = nn.Linear(100, 50)\n",
    "        self.BN2 = nn.BatchNorm1d(50)\n",
    "        self.LinearLayer3 = nn.Linear(50, 33)\n",
    "        self.BN3 = nn.BatchNorm1d(33)\n",
    "        self.LinearLayer4 = nn.Linear(33, 1)\n",
    "        self.ELUActivation = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = self.LinearLayer1(x)\n",
    "        bn1 = self.BN1(z1)\n",
    "        a1 = self.ELUActivation(bn1)\n",
    "\n",
    "        z2 = self.LinearLayer2(a1)\n",
    "        bn2 = self.BN2(z2)\n",
    "        a2 = self.ELUActivation(bn2)\n",
    "\n",
    "        z3 = self.LinearLayer3(a2)\n",
    "        bn3 = self.BN3(z3)\n",
    "        a3 = self.ELUActivation(bn3)\n",
    "\n",
    "        z4 = self.LinearLayer4(a3)\n",
    "\n",
    "        return z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404d9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        #input size = 121\n",
    "        #Use Gated Recurrent Unit (GRU)\n",
    "        #Initialize h and n to 0s\n",
    "        #Hidden state dim = 20\n",
    "        #Stacking GRU cells = 10?\n",
    "        #Hidden state of the top most GRU is linked to a FCL\n",
    "        #RMSProp optimizer\n",
    "        #Learning rate = 0.001\n",
    "        #Epochs = 5\n",
    "        #Batch size = 128\n",
    "        \n",
    "        self.device = device\n",
    "        self.input_size = 121\n",
    "        self.num_layers = 4\n",
    "        self.hidden_size = 20\n",
    "        self.num_classes = 1\n",
    "        self.rnn = nn.GRU(self.input_size, self.hidden_size, self.num_layers, batch_first = True)\n",
    "        # x -> (batch_size, sequence_size, input_size)\n",
    "        self.linear_layer = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        initial_hidden_state = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "\n",
    "        out, _ = self.rnn(self, initial_hidden_state)\n",
    "        # out: batch_size, sequence_length, hidden_size\n",
    "\n",
    "        #out: (batch_size, hidden_size)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.linear_layer(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4acecc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_per_epsilon(losses, BHAR):\n",
    "    data = pd.merge(losses, BHAR, how=\"outer\", left_index=True, right_index = True)\n",
    "    filt_000 = (data['BHAR'].abs() > 0) & (data['BHAR'].abs() <= 0.05)\n",
    "    filt_005 = (data['BHAR'].abs() > 0.05) & (data['BHAR'].abs() <= 0.10)\n",
    "    filt_010 = (data['BHAR'].abs() > 0.10) & (data['BHAR'].abs() <= 0.20)\n",
    "    filt_020 = (data['BHAR'].abs() > 0.20) & (data['BHAR'].abs() <= 0.50)\n",
    "    filt_050 = (data['BHAR'].abs() > 0.50) & (data['BHAR'].abs() <= 1)\n",
    "    filt_100 = (data['BHAR'].abs() > 1)\n",
    "\n",
    "    print(f'0 Error: {data.loc[filt_000].shape}\\n 0.05 Error: {data.loc[filt_005].shape}\\n 0.10 Error: {data.loc[filt_010].shape}\\n 0.20 Error: {data.loc[filt_020].shape}\\n 0.50 Error: {data.loc[filt_050].shape}\\n 1.00 Error: {data.loc[filt_100].shape}\\n')\n",
    "\n",
    "\n",
    "    data_000 = data.loc[filt_000][\"Loss\"].mean()\n",
    "    data_005 = data.loc[filt_005][\"Loss\"].mean()\n",
    "    data_010 = data.loc[filt_010][\"Loss\"].mean()\n",
    "    data_020 = data.loc[filt_020][\"Loss\"].mean()\n",
    "    data_050 = data.loc[filt_050][\"Loss\"].mean()\n",
    "    data_100 = data.loc[filt_100][\"Loss\"].mean()\n",
    "\n",
    "    print(f'0 Error: {data_000}\\n 0.05 Error: {data_005}\\n 0.10 Error: {data_010}\\n 0.20 Error: {data_020}\\n 0.50 Error: {data_050}\\n 1.00 Error: {data_100}\\n')\n",
    "\n",
    "def percentage_correct(outs, BHAR):\n",
    "    data = pd.merge(outs, BHAR, how=\"outer\", left_index=True, right_index = True)\n",
    "    filt = ((data['BHAR'] > 0)  & (data['Out'] > 0) | (data['BHAR'] < 0)  & (data['Out'] < 0))\n",
    "    filt = pd.DataFrame(filt, columns=['Percentage Correct'])\n",
    "    print(f'Percentage_correct: {filt[\"Percentage Correct\"].value_counts(normalize=True)}')\n",
    "    return filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e46ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Machine Learning-Based Financial Statement Analysis')\n",
    "parser.add_argument('--config', default='./configs/config.yaml')\n",
    "\n",
    "class QuarterlyFundamentalData(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        dataset = np.loadtxt(filename, delimiter=\",\")\n",
    "        self.x = torch.from_numpy(dataset[:, :484]) # Skip the column that is the target\n",
    "        self.y = torch.from_numpy(dataset[:, [484]]) # Size = (n_samples, 1)\n",
    "        self.num_samples = dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    batch_size = target.shape[0]\n",
    "\n",
    "    _, pred = torch.max(output, dim=-1)\n",
    "\n",
    "    correct = pred.eq(target).sum() * 1.0\n",
    "\n",
    "    acc = correct / batch_size\n",
    "\n",
    "    return acc\n",
    "\n",
    "def ML_train(epoch, data_loader, model):\n",
    "    iter_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    \n",
    "\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        start = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        data = data.cpu().float().numpy()\n",
    "        target = target.cpu().float().numpy()\n",
    "        model.train(data, target)\n",
    "        iter_time.update(time.time() - start)\n",
    "\n",
    "\n",
    "def ML_validation(epoch, val_loader, model, criterion, percentage_correct_criterion):\n",
    "    iter_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    all_losses = []\n",
    "    all_outs = []\n",
    "\n",
    "    for idx, (data, target) in enumerate(val_loader):\n",
    "        start = time.time()\n",
    "        data = data.cpu().float().numpy()\n",
    "        target = target.float()\n",
    "\n",
    "        out = torch.tensor(model.test(data))#.unsqueeze(1)\n",
    "        loss = criterion(out, target)\n",
    "        rmse = torch.sqrt(loss)\n",
    "        medae = torch.median(nn.L1Loss(reduction='none')(target, out))\n",
    "\n",
    "        all_losses.append(percentage_correct_criterion(out, target).squeeze(1).tolist())\n",
    "        all_outs.append(out)\n",
    "        batch_acc = accuracy(out, target)\n",
    "\n",
    "        losses.update(loss, out.shape[0])\n",
    "        acc.update(batch_acc, out.shape[0])\n",
    "\n",
    "        iter_time.update(time.time() - start)\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                   'Time {iter_time.val:.3f} ({iter_time.avg:.3f})\\t'\n",
    "                   'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                   'Prec @1 {top1.val:.4f} ({top1.avg:.4f})\\t')\n",
    "                  .format(epoch, idx, len(val_loader), iter_time=iter_time, loss=losses, top1=acc))\n",
    "            \n",
    "    print(f'RSME: ', rmse)\n",
    "    print(f'MEDAE: ', medae)\n",
    "\n",
    "    return all_losses, all_outs, losses.avg.tolist()\n",
    "    \n",
    "\n",
    "def train(epoch, data_loader, model, optimizer, criterion):\n",
    "    iter_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        start = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        data = data.float()\n",
    "        target = target.float()\n",
    "        out = model.forward(data)\n",
    "        loss = criterion(out, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_acc = accuracy(out, target)\n",
    "\n",
    "        losses.update(loss, out.shape[0])\n",
    "        acc.update(batch_acc, out.shape[0])\n",
    "\n",
    "        iter_time.update(time.time() - start)\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                   'Time {iter_time.val:.3f} ({iter_time.avg:.3f})\\t'\n",
    "                   'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                   'Prec @1 {top1.val:.4f} ({top1.avg:.4f})\\t')\n",
    "                  .format(epoch, idx, len(data_loader), iter_time=iter_time, loss=losses, top1=acc))\n",
    "\n",
    "    return losses.avg.tolist()\n",
    "\n",
    "def validate(epoch, val_loader, model, criterion, percentage_correct_criterion):\n",
    "    iter_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    all_losses = []\n",
    "    all_outs = []\n",
    "\n",
    "    num_class = 1\n",
    "    cm = torch.zeros(num_class, num_class)\n",
    "    for idx, (data, target) in enumerate(val_loader):\n",
    "        start = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        data = data.float()\n",
    "        target = target.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(data)\n",
    "            loss = criterion(out, target)\n",
    "            rmse = torch.sqrt(loss)\n",
    "            medae = torch.median(nn.L1Loss(reduction='none')(target, out))\n",
    "        all_losses.append(percentage_correct_criterion(out, target).squeeze(1).tolist())\n",
    "        all_outs.append(out)\n",
    "        batch_acc = accuracy(out, target)\n",
    "\n",
    "        \n",
    "\n",
    "        # update confusion matrix\n",
    "        _, preds = torch.max(out, 1)\n",
    "        losses.update(loss, out.shape[0])\n",
    "        acc.update(batch_acc, out.shape[0])\n",
    "\n",
    "        iter_time.update(time.time() - start)\n",
    "        if idx % 10 == 0:\n",
    "            print(('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                   'Time {iter_time.val:.3f} ({iter_time.avg:.3f})\\t'\n",
    "                   'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                   'Prec @1 {top1.val:.4f} ({top1.avg:.4f})\\t')\n",
    "                  .format(epoch, idx, len(val_loader), iter_time=iter_time, loss=losses, top1=acc))\n",
    "\n",
    "    print(f'RSME: ', rmse)   \n",
    "    print(f'MEDAE: ', medae)\n",
    "    \n",
    "    return all_losses, all_outs, losses.avg.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 4096\n",
    "learning_rate = 0.00005\n",
    "train_set_filename = 'data/batched_train_data.csv'\n",
    "val_set_filename = 'data/batched_val_data.csv'\n",
    "model = OLS()#.to(device)\n",
    "#model = DNN().to(device)\n",
    "#dataset = np.loadtxt('/content/drive/MyDrive/Machine-Learning-Based-Financial-Statement-Analysis/data/batched_train_data.csv', delimiter=\",\")\n",
    "#data = torch.from_numpy(dataset[:, :484]) \n",
    "#target = torch.from_numpy(dataset[:, [484]]) \n",
    "\n",
    "\n",
    "\n",
    "val_dataset = np.loadtxt('data/batched_val_data.csv', delimiter=\",\")\n",
    "#val_data = torch.from_numpy(val_dataset[:, :484]) \n",
    "val_target = torch.from_numpy(val_dataset[:, [484]]) \n",
    "\n",
    "train_dataset = QuarterlyFundamentalData(train_set_filename)\n",
    "data_loader = DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle=False, num_workers=2) # num_workers uses multiple subprocesses\n",
    "\n",
    "val_dataset = QuarterlyFundamentalData(val_set_filename)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size= batch_size, shuffle=False, num_workers=2) # num_workers uses multiple subprocesses\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "percentage_correct_criterion = nn.MSELoss(reduction='none')\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_all_losses = []\n",
    "val_percentage_correct = []\n",
    "val_all_outs = []\n",
    "#filename= '/content/drive/MyDrive/Machine-Learning-Based-Financial-Statement-Analysis/trained_models/ols_model_mse.sav'\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#model.set_model(loaded_model)\n",
    "##result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Train the model\n",
    "    ML_train(epoch, data_loader, model)\n",
    "    val_all_losses, val_all_outs, tmp = ML_validation(epoch, val_loader, model, criterion, percentage_correct_criterion)\n",
    "    val_losses.append(tmp)\n",
    "\n",
    "\n",
    "    #train_losses.append(train(epoch, data_loader, model, optimizer, criterion))\n",
    "    #val_all_losses, val_all_outs, tmp = validate(epoch, val_loader, model, criterion, percentage_correct_criterion)\n",
    "    #val_losses.append(tmp)\n",
    "\n",
    "#print(f'Train: {train_losses}')\n",
    "\n",
    "print(f'Val: {val_losses}')\n",
    "\n",
    "\n",
    "\n",
    "BHAR = pd.DataFrame(val_target.tolist(), columns=[\"BHAR\"])\n",
    "losses = [item for sublist in val_all_losses for item in sublist]\n",
    "val_all_losses = pd.DataFrame(losses, columns=[\"Loss\"])\n",
    "\n",
    "val_all_outs = [item for sublist in val_all_outs for item in sublist]\n",
    "val_all_outs = pd.DataFrame(val_all_outs, columns=[\"Out\"])\n",
    "\n",
    "calculate_error_per_epsilon(val_all_losses, BHAR)\n",
    "percentage_correct(val_all_outs, BHAR)\n",
    "\"\"\"\n",
    "data_columns = pd.read_csv('data/train_data.csv', nrows=1).columns\n",
    "data_columns = list(data_columns.drop(['tic', 'datadate', 'PRC', 'BHAR']))\n",
    "print(f'All Columns: {type(data_columns)}, RF Features: {type(model.random_forest_fitted.feature_importances_.shape)}')\n",
    "all_columns = data_columns + data_columns + data_columns + data_columns\n",
    "sorted_idx = model.random_forest_fitted.feature_importances_.argsort()[-10:]\n",
    "\n",
    "print(f'Sorted Index: {sorted_idx}, All Columns: {np.array(all_columns)[sorted_idx.astype(int)]}, RF Features: {np.array(model.random_forest_fitted.feature_importances_)[sorted_idx.astype(int)]}')\n",
    "plt.barh(np.array(all_columns)[sorted_idx.astype(int)], np.array(model.random_forest_fitted.feature_importances_)[sorted_idx.astype(int)])\n",
    "\n",
    "plt.title(\"Important Features\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "#plt.legend([\"Train\", \"Val\"])\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"Train\", \"Val\"])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
